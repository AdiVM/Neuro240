{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNhvwhBbhF+Cn1a0CI/kvfK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdiVM/Neuro240/blob/main/Neuro240FinalProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_PdcJlQyDlD",
        "outputId": "fe63401c-17bf-4c37-cc09-31161ebed289"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# All future runs can start here\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "import pandas as pd\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metadata_path = \"/content/drive/MyDrive/NIH_ChestXRay_Data_Neuro240/Data_Entry_2017_v2020.csv\"\n",
        "image_folder = \"/content/drive/MyDrive/NIH_ChestXRay_Data_Neuro240/images\"\n",
        "\n",
        "metadata = pd.read_csv(metadata_path)\n",
        "\n",
        "print(\"Metdata loaded\")\n",
        "\n",
        "# Filtering the metadata to find images labeled either no finding or those containing the word mass\n",
        "filtered_metadata = metadata[\n",
        "    (metadata[\"Finding Labels\"] == \"No Finding\") |\n",
        "    (metadata[\"Finding Labels\"].str.contains(\"Mass\", na=False))\n",
        "]\n",
        "\n",
        "filtered_image_indexes = set(filtered_metadata[\"Image Index\"])\n",
        "filtered_metadata = filtered_metadata.head(50000)\n",
        "\n",
        "matching_images = sorted(list(filtered_metadata[\"Image Index\"]))\n",
        "\n",
        "# Convert to stored list\n",
        "matching_images = sorted(list(matching_images))\n",
        "\n",
        "print(f\"Total matching images found: {len(matching_images)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8-T_YFvyEpR",
        "outputId": "05c9201b-20c5-4a73-95d2-7f07f0589f0c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metdata loaded\n",
            "Total matching images found: 50000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now to perform stratified shuffle split\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        " # Check class distribution before splitting\n",
        "print(filtered_metadata[\"Finding Labels\"].value_counts())\n",
        "\n",
        "# There are many small classes of mass, so need to group them all together before splitting\n",
        "# Standardize labels: Convert anything containing \"Mass\" to just \"Mass\"\n",
        "filtered_metadata[\"Finding Labels\"] = filtered_metadata[\"Finding Labels\"].apply(\n",
        "    lambda x: \"Mass\" if \"Mass\" in x else x\n",
        ")\n",
        "\n",
        "# Verify new label counts\n",
        "print(filtered_metadata[\"Finding Labels\"].value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xKqoPmUyLE-",
        "outputId": "dc5687a1-7306-4454-e12c-18dda0dc7b6e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finding Labels\n",
            "No Finding                                                                          45542\n",
            "Mass                                                                                 1708\n",
            "Infiltration|Mass                                                                     329\n",
            "Mass|Nodule                                                                           293\n",
            "Effusion|Mass                                                                         285\n",
            "                                                                                    ...  \n",
            "Effusion|Emphysema|Mass|Nodule                                                          1\n",
            "Atelectasis|Consolidation|Effusion|Fibrosis|Infiltration|Mass|Pleural_Thickening        1\n",
            "Atelectasis|Consolidation|Effusion|Fibrosis|Infiltration|Mass                           1\n",
            "Cardiomegaly|Consolidation|Effusion|Infiltration|Mass|Nodule                            1\n",
            "Edema|Fibrosis|Infiltration|Mass                                                        1\n",
            "Name: count, Length: 258, dtype: int64\n",
            "Finding Labels\n",
            "No Finding    45542\n",
            "Mass           4458\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_map = {\"No Finding\": 0, \"Mass\": 1}\n",
        "filtered_metadata[\"Label\"] = filtered_metadata[\"Finding Labels\"].map(label_map)"
      ],
      "metadata": {
        "id": "zxHdem9r6Ewk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data while ensuring proportional distribution of classes\n",
        "train_metadata, test_metadata = train_test_split(\n",
        "    filtered_metadata,\n",
        "    test_size=0.2,\n",
        "    stratify=filtered_metadata[\"Label\"],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Class distribution in train and test sets\n",
        "print(\"Training set:\")\n",
        "print(train_metadata[\"Label\"].value_counts())\n",
        "\n",
        "print(\"Testing Set:\")\n",
        "print(test_metadata[\"Label\"].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xripsguQyPqP",
        "outputId": "07b17097-ddb1-4a43-9956-ccbe112566aa"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set:\n",
            "Label\n",
            "0    36434\n",
            "1     3566\n",
            "Name: count, dtype: int64\n",
            "Testing Set:\n",
            "Label\n",
            "0    9108\n",
            "1     892\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Train metadata entries: {len(train_metadata)}\")\n",
        "print(f\"Test metadata entries: {len(test_metadata)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6h6-YhWySET",
        "outputId": "6cd81183-8ded-4975-aed1-79c90682b95a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train metadata entries: 40000\n",
            "Test metadata entries: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtering metdata\n",
        "# Convert \"Image Index\" column to a set for fast lookup\n",
        "train_image_files = set(train_metadata[\"Image Index\"])\n",
        "test_image_files = set(test_metadata[\"Image Index\"])\n",
        "\n",
        "train_images = sorted(list(train_image_files))\n",
        "test_images = sorted(list(test_image_files))\n",
        "\n",
        "# Convert to sorted lists for consistency\n",
        "train_images = sorted(list(train_images))\n",
        "test_images = sorted(list(test_images))\n",
        "\n",
        "\n",
        "print(f\"Total train images found: {len(train_images)}\")\n",
        "print(f\"Total test images found: {len(test_images)}\")\n",
        "\n",
        "# Print a few samples\n",
        "print(\"Sample train images:\", train_images[:10])\n",
        "print(\"Sample test images:\", test_images[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c-cLJqxySmw",
        "outputId": "25988ee2-2383-449f-8f06-f3e1075b18da"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total train images found: 40000\n",
            "Total test images found: 10000\n",
            "Sample train images: ['00000002_000.png', '00000004_000.png', '00000005_000.png', '00000005_002.png', '00000005_003.png', '00000005_004.png', '00000005_005.png', '00000006_000.png', '00000007_000.png', '00000011_002.png']\n",
            "Sample test images: ['00000005_001.png', '00000008_001.png', '00000011_001.png', '00000011_003.png', '00000013_000.png', '00000013_017.png', '00000013_029.png', '00000013_030.png', '00000018_000.png', '00000032_049.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Will use TensorFlow for model training\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "JSullbIDyWwk"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Image preprocessing parameters\n",
        "image_size = (224, 224)  # Resize images\n",
        "batch_size = 32\n",
        "\n",
        "# Data augmentation for training\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1.0 / 255,  # Normalize pixel values\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "# Only rescale for testing\n",
        "test_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
        "\n",
        "# Load train images from directory\n",
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "    dataframe=train_metadata,\n",
        "    directory=image_folder,\n",
        "    x_col=\"Image Index\",\n",
        "    y_col=\"Label\",\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"raw\"\n",
        ")\n",
        "\n",
        "# Load test images\n",
        "test_generator = test_datagen.flow_from_dataframe(\n",
        "    dataframe=test_metadata,\n",
        "    directory=image_folder,\n",
        "    x_col=\"Image Index\",\n",
        "    y_col=\"Label\",\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"raw\"\n",
        ")"
      ],
      "metadata": {
        "id": "gaTNWs9lyY6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train distribution:\")\n",
        "print(train_metadata[\"Label\"].value_counts())\n",
        "\n",
        "print(\"\\nTest distribution:\")\n",
        "print(test_metadata[\"Label\"].value_counts())"
      ],
      "metadata": {
        "id": "lApFIyKt6dYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def train_model_with_subset(train_metadata, test_metadata, image_folder,\n",
        "#                             train_size,\n",
        "#                             image_size=(224, 224), batch_size=32, epochs=5,\n",
        "#                             output_dir=\"/content/drive/MyDrive/NIH_ChestXRay_Data_Neuro240/results\"):\n",
        "#     import os\n",
        "#     import pandas as pd\n",
        "#     import matplotlib.pyplot as plt\n",
        "#     import seaborn as sns\n",
        "#     from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve\n",
        "#     from tensorflow.keras.models import Sequential\n",
        "#     from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "#     from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "#     # Sample training and test subsets\n",
        "#     train_metadata_subset = train_metadata.sample(train_size, random_state=42)\n",
        "#     test_metadata_subset = test_metadata.sample(int(train_size / 4), random_state=42)\n",
        "\n",
        "#     os.makedirs(output_dir, exist_ok=True)\n",
        "#     result_prefix = os.path.join(output_dir, f\"train_{train_size}\")\n",
        "\n",
        "#     # Save class distributions\n",
        "#     def save_distribution(df, name):\n",
        "#         counts = df[\"Finding Labels\"].value_counts()\n",
        "#         percents = counts / counts.sum() * 100\n",
        "#         dist_df = pd.DataFrame({\"Count\": counts, \"Percent\": percents})\n",
        "#         dist_df.to_csv(f\"{result_prefix}_{name}_distribution.csv\")\n",
        "#         return dist_df\n",
        "\n",
        "#     train_dist = save_distribution(train_metadata_subset, \"train\")\n",
        "#     test_dist = save_distribution(test_metadata_subset, \"test\")\n",
        "\n",
        "#     # Combined barplot\n",
        "#     combined_df = pd.concat([\n",
        "#         train_dist[\"Percent\"].rename(\"Train\"),\n",
        "#         test_dist[\"Percent\"].rename(\"Test\")\n",
        "#     ], axis=1).fillna(0).reset_index()\n",
        "\n",
        "#     # Rename the correct column to 'Label' (usually the first column)\n",
        "#     combined_df.columns.values[0] = \"Label\"\n",
        "\n",
        "#     combined_df = pd.melt(combined_df, id_vars=\"Label\", var_name=\"Set\", value_name=\"Percent\")\n",
        "\n",
        "#     plt.figure(figsize=(6, 4))\n",
        "#     sns.barplot(data=combined_df, x=\"Label\", y=\"Percent\", hue=\"Set\")\n",
        "#     plt.title(\"Class Distribution: Train vs Test\")\n",
        "#     plt.ylabel(\"Percentage\")\n",
        "#     plt.xlabel(\"Class Label\")\n",
        "#     plt.tight_layout()\n",
        "#     plt.savefig(f\"{result_prefix}_class_distribution_comparison.png\")\n",
        "#     plt.close()\n",
        "\n",
        "#     # Image data generators\n",
        "#     train_datagen = ImageDataGenerator(\n",
        "#         rescale=1.0 / 255,\n",
        "#         rotation_range=15,\n",
        "#         width_shift_range=0.1,\n",
        "#         height_shift_range=0.1,\n",
        "#         horizontal_flip=True\n",
        "#     )\n",
        "#     test_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
        "\n",
        "#     train_generator = train_datagen.flow_from_dataframe(\n",
        "#         dataframe=train_metadata_subset,\n",
        "#         directory=image_folder,\n",
        "#         x_col=\"Image Index\",\n",
        "#         y_col=\"Finding Labels\",\n",
        "#         target_size=image_size,\n",
        "#         batch_size=batch_size,\n",
        "#         class_mode=\"binary\",\n",
        "#         shuffle=True\n",
        "#     )\n",
        "#     test_generator = test_datagen.flow_from_dataframe(\n",
        "#         dataframe=test_metadata_subset,\n",
        "#         directory=image_folder,\n",
        "#         x_col=\"Image Index\",\n",
        "#         y_col=\"Finding Labels\",\n",
        "#         target_size=image_size,\n",
        "#         batch_size=batch_size,\n",
        "#         class_mode=\"binary\",\n",
        "#         shuffle=False\n",
        "#     )\n",
        "\n",
        "#     # CNN model\n",
        "#     model = Sequential([\n",
        "#         Conv2D(32, (3, 3), activation=\"relu\", input_shape=(224, 224, 3)),\n",
        "#         MaxPooling2D(pool_size=(2, 2)),\n",
        "#         Conv2D(64, (3, 3), activation=\"relu\"),\n",
        "#         MaxPooling2D(pool_size=(2, 2)),\n",
        "#         Conv2D(128, (3, 3), activation=\"relu\"),\n",
        "#         MaxPooling2D(pool_size=(2, 2)),\n",
        "#         Flatten(),\n",
        "#         Dense(128, activation=\"relu\"),\n",
        "#         Dropout(0.5),\n",
        "#         Dense(1, activation=\"sigmoid\")\n",
        "#     ])\n",
        "\n",
        "#     model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "#     history = model.fit(\n",
        "#         train_generator,\n",
        "#         validation_data=test_generator,\n",
        "#         epochs=epochs,\n",
        "#         verbose=1\n",
        "#     )\n",
        "\n",
        "#     test_loss, test_acc = model.evaluate(test_generator)\n",
        "#     y_pred_proba = model.predict(test_generator).flatten()\n",
        "#     y_pred = (y_pred_proba > 0.5).astype(int)\n",
        "#     y_true = test_generator.classes\n",
        "#     auc = roc_auc_score(y_true, y_pred_proba)\n",
        "#     cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "#     # Save confusion matrix\n",
        "#     confusion_df = pd.DataFrame(\n",
        "#     cm,\n",
        "#     index=[\"Mass\", \"No Finding\"],\n",
        "#     columns=[\"Predicted Mass\", \"Predicted No Finding\"])\n",
        "\n",
        "#     confusion_df.to_csv(f\"{result_prefix}_confusion_matrix.csv\")\n",
        "\n",
        "#     # Save predictions\n",
        "#     filenames = test_generator.filenames\n",
        "#     results_df = pd.DataFrame({\n",
        "#         \"Filename\": filenames,\n",
        "#         \"TrueLabel\": y_true,\n",
        "#         \"PredictedLabel\": y_pred,\n",
        "#         \"PredictedProb\": y_pred_proba\n",
        "#     })\n",
        "#     results_df.to_csv(f\"{result_prefix}_predictions.csv\", index=False)\n",
        "\n",
        "#     # Save ROC curve\n",
        "#     fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n",
        "#     plt.figure()\n",
        "#     plt.plot(fpr, tpr, label=f\"AUC = {auc:.2f}\")\n",
        "#     plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")\n",
        "#     plt.xlabel(\"False Positive Rate\")\n",
        "#     plt.ylabel(\"True Positive Rate\")\n",
        "#     plt.title(\"ROC Curve\")\n",
        "#     plt.legend()\n",
        "#     plt.grid(True)\n",
        "#     plt.savefig(f\"{result_prefix}_roc_curve.png\")\n",
        "#     plt.close()\n",
        "\n",
        "#     print(f\"Test Accuracy (train size={train_size}): {test_acc * 100:.2f}%\")\n",
        "#     print(f\"AUC: {auc:.4f}\")\n",
        "\n",
        "#     return model, history, test_acc, auc"
      ],
      "metadata": {
        "id": "_F0GVzmCyda9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model, history, acc, auc = train_model_with_subset(\n",
        "#     train_metadata=train_metadata,\n",
        "#     test_metadata=test_metadata,\n",
        "#     image_folder=image_folder,\n",
        "#     train_size=500\n",
        "# )"
      ],
      "metadata": {
        "id": "GCDAtHGyyeAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_250, history_250, acc_250, auc_250 = train_model_with_subset(\n",
        "#     train_metadata=train_metadata,\n",
        "#     test_metadata=test_metadata,\n",
        "#     image_folder=image_folder,\n",
        "#     train_size=250\n",
        "# )"
      ],
      "metadata": {
        "id": "Dt27vdnmyzGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_1000, history_1000, acc_1000, auc_1000 = train_model_with_subset(\n",
        "#     train_metadata=train_metadata,\n",
        "#     test_metadata=test_metadata,\n",
        "#     image_folder=image_folder,\n",
        "#     train_size=1000\n",
        "# )"
      ],
      "metadata": {
        "id": "9XlYdBJFyzj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_250, history_250, acc_250, auc_250 = train_model_with_noise(\n",
        "#     train_metadata=train_metadata,\n",
        "#     test_metadata=test_metadata,\n",
        "#     image_folder=image_folder,\n",
        "#     train_size=500\n",
        "# )"
      ],
      "metadata": {
        "id": "V7f0cO8Sy4A_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def train_model_with_class_weighting(train_metadata, test_metadata, image_folder,\n",
        "#                             train_size,\n",
        "#                             image_size=(224, 224), batch_size=32, epochs=5,\n",
        "#                             output_dir=\"/content/drive/MyDrive/NIH_ChestXRay_Data_Neuro240/results_class_weight\"):\n",
        "#     import os\n",
        "#     import pandas as pd\n",
        "#     import matplotlib.pyplot as plt\n",
        "#     import seaborn as sns\n",
        "#     import numpy as np\n",
        "#     from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve\n",
        "#     from tensorflow.keras.models import Sequential\n",
        "#     from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "#     from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "#     # Sample training and test subsets\n",
        "#     half_size = train_size // 2\n",
        "#     mass_subset = train_metadata[train_metadata[\"Finding Labels\"] == \"Mass\"].sample(half_size, random_state=42)\n",
        "#     no_finding_subset = train_metadata[train_metadata[\"Finding Labels\"] == \"No Finding\"].sample(half_size, random_state=42)\n",
        "#     train_metadata_subset = pd.concat([mass_subset, no_finding_subset]).sample(frac=1, random_state=42)  # shuffle\n",
        "\n",
        "#     test_metadata_subset = test_metadata.sample(int(train_size / 4), random_state=42)\n",
        "\n",
        "#     os.makedirs(output_dir, exist_ok=True)\n",
        "#     result_prefix = os.path.join(output_dir, f\"train_{train_size}\")\n",
        "\n",
        "#     # Save class distributions\n",
        "#     def save_distribution(df, name):\n",
        "#         counts = df[\"Finding Labels\"].value_counts()\n",
        "#         percents = counts / counts.sum() * 100\n",
        "#         dist_df = pd.DataFrame({\"Count\": counts, \"Percent\": percents})\n",
        "#         dist_df.to_csv(f\"{result_prefix}_{name}_distribution.csv\")\n",
        "#         return dist_df\n",
        "\n",
        "#     train_dist = save_distribution(train_metadata_subset, \"train\")\n",
        "#     test_dist = save_distribution(test_metadata_subset, \"test\")\n",
        "\n",
        "#     # Combined barplot\n",
        "#     combined_df = pd.concat([\n",
        "#         train_dist[\"Percent\"].rename(\"Train\"),\n",
        "#         test_dist[\"Percent\"].rename(\"Test\")\n",
        "#     ], axis=1).fillna(0).reset_index()\n",
        "\n",
        "#     # Rename the correct column to 'Label' (usually the first column)\n",
        "#     combined_df.columns.values[0] = \"Label\"\n",
        "\n",
        "#     combined_df = pd.melt(combined_df, id_vars=\"Label\", var_name=\"Set\", value_name=\"Percent\")\n",
        "\n",
        "#     plt.figure(figsize=(6, 4))\n",
        "#     sns.barplot(data=combined_df, x=\"Label\", y=\"Percent\", hue=\"Set\")\n",
        "#     plt.title(\"Class Distribution: Train vs Test\")\n",
        "#     plt.ylabel(\"Percentage\")\n",
        "#     plt.xlabel(\"Class Label\")\n",
        "#     plt.tight_layout()\n",
        "#     plt.savefig(f\"{result_prefix}_class_distribution_comparison.png\")\n",
        "#     plt.close()\n",
        "\n",
        "#     # Image data generators\n",
        "#     train_datagen = ImageDataGenerator(\n",
        "#         rescale=1.0 / 255,\n",
        "#         rotation_range=15,\n",
        "#         width_shift_range=0.1,\n",
        "#         height_shift_range=0.1,\n",
        "#         horizontal_flip=False\n",
        "#     )\n",
        "#     test_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
        "\n",
        "#     train_generator = train_datagen.flow_from_dataframe(\n",
        "#         dataframe=train_metadata_subset,\n",
        "#         directory=image_folder,\n",
        "#         x_col=\"Image Index\",\n",
        "#         y_col=\"Finding Labels\",\n",
        "#         target_size=image_size,\n",
        "#         batch_size=batch_size,\n",
        "#         class_mode=\"binary\",\n",
        "#         shuffle=False\n",
        "#     )\n",
        "#     test_generator = test_datagen.flow_from_dataframe(\n",
        "#         dataframe=test_metadata_subset,\n",
        "#         directory=image_folder,\n",
        "#         x_col=\"Image Index\",\n",
        "#         y_col=\"Finding Labels\",\n",
        "#         target_size=image_size,\n",
        "#         batch_size=batch_size,\n",
        "#         class_mode=\"binary\",\n",
        "#         shuffle=False\n",
        "#     )\n",
        "\n",
        "#     # CNN model\n",
        "#     model = Sequential([\n",
        "#         Conv2D(32, (3, 3), activation=\"relu\", input_shape=(224, 224, 3)),\n",
        "#         MaxPooling2D(pool_size=(2, 2)),\n",
        "#         Conv2D(64, (3, 3), activation=\"relu\"),\n",
        "#         MaxPooling2D(pool_size=(2, 2)),\n",
        "#         Conv2D(128, (3, 3), activation=\"relu\"),\n",
        "#         MaxPooling2D(pool_size=(2, 2)),\n",
        "#         Flatten(),\n",
        "#         Dense(128, activation=\"relu\"),\n",
        "#         Dropout(0.5),\n",
        "#         Dense(1, activation=\"sigmoid\")\n",
        "#     ])\n",
        "\n",
        "#     model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "#     history = model.fit(\n",
        "#     train_generator,\n",
        "#     validation_data=test_generator,\n",
        "#     epochs=epochs,\n",
        "#     verbose=1)\n",
        "\n",
        "#     test_loss, test_acc = model.evaluate(test_generator)\n",
        "#     y_pred_proba = model.predict(test_generator).flatten()\n",
        "#     from sklearn.metrics import f1_score\n",
        "\n",
        "#     # Trying multiple thresholds to find the optimal one\n",
        "#     y_true = test_generator.classes\n",
        "\n",
        "#     thresholds = np.linspace(0.1, 0.9, 9)\n",
        "#     best_f1 = 0\n",
        "#     best_threshold = 0.5  # default\n",
        "#     best_preds = None\n",
        "\n",
        "#     for t in thresholds:\n",
        "#       preds = (y_pred_proba > t).astype(int)\n",
        "#       f1 = f1_score(y_true, preds)\n",
        "#       print(f\"Threshold {t:.2f} → F1 Score: {f1:.4f}\")\n",
        "#       if f1 > best_f1:\n",
        "#           best_f1 = f1\n",
        "#           best_threshold = t\n",
        "#           best_preds = preds\n",
        "\n",
        "#     print(f\"\\nBest Threshold: {best_threshold:.2f} → F1 Score: {best_f1:.4f}\")\n",
        "\n",
        "\n",
        "#     auc = roc_auc_score(y_true, y_pred_proba)\n",
        "#     cm = confusion_matrix(y_true, best_preds)\n",
        "\n",
        "#     # Save confusion matrix\n",
        "#     confusion_df = pd.DataFrame(\n",
        "#     cm,\n",
        "#     index=[\"Mass\", \"No Finding\"],\n",
        "#     columns=[\"Predicted Mass\", \"Predicted No Finding\"])\n",
        "\n",
        "#     confusion_df.to_csv(f\"{result_prefix}_confusion_matrix.csv\")\n",
        "\n",
        "#     # Save predictions\n",
        "#     filenames = test_generator.filenames\n",
        "#     results_df = pd.DataFrame({\n",
        "#         \"Filename\": filenames,\n",
        "#         \"TrueLabel\": y_true,\n",
        "#         \"PredictedLabel\": best_preds,\n",
        "#         \"PredictedProb\": y_pred_proba\n",
        "#     })\n",
        "#     results_df.to_csv(f\"{result_prefix}_predictions.csv\", index=False)\n",
        "\n",
        "#     # Save ROC curve\n",
        "#     fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n",
        "#     plt.figure()\n",
        "#     plt.plot(fpr, tpr, label=f\"AUC = {auc:.2f}\")\n",
        "#     plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")\n",
        "#     plt.xlabel(\"False Positive Rate\")\n",
        "#     plt.ylabel(\"True Positive Rate\")\n",
        "#     plt.title(\"ROC Curve\")\n",
        "#     plt.legend()\n",
        "#     plt.grid(True)\n",
        "#     plt.savefig(f\"{result_prefix}_roc_curve.png\")\n",
        "#     plt.close()\n",
        "\n",
        "#     print(f\"Test Accuracy (train size={train_size}): {test_acc * 100:.2f}%\")\n",
        "#     print(f\"AUC: {auc:.4f}\")\n",
        "\n",
        "#     with open(f\"{result_prefix}_best_threshold.txt\", \"w\") as f:\n",
        "#       f.write(f\"Best threshold: {best_threshold:.2f}, F1: {best_f1:.4f}\")\n",
        "\n",
        "#     return model, history, test_acc, auc"
      ],
      "metadata": {
        "id": "We89N-5FJo92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_250_class, history_250_class, acc_250_class, auc_250_class = train_model_with_class_weighting(\n",
        "#     train_metadata=train_metadata,\n",
        "#     test_metadata=test_metadata,\n",
        "#     image_folder=image_folder,\n",
        "#     train_size=250\n",
        "# )"
      ],
      "metadata": {
        "id": "c9HfxrncMS0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_500_class, history_500_class, acc_500_class, auc_500_class = train_model_with_class_weighting(\n",
        "#     train_metadata=train_metadata,\n",
        "#     test_metadata=test_metadata,\n",
        "#     image_folder=image_folder,\n",
        "#     train_size=500\n",
        "# )"
      ],
      "metadata": {
        "id": "_0JwiWZiQm-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_500_class, history_500_class, acc_500_class, auc_500_class = train_model_with_class_weighting(\n",
        "#     train_metadata=train_metadata,\n",
        "#     test_metadata=test_metadata,\n",
        "#     image_folder=image_folder,\n",
        "#     train_size=7000\n",
        "# )"
      ],
      "metadata": {
        "id": "aFW6e1tOEHjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_1000_class, history_1000_class, acc_1000_class, auc_1000_class = train_model_with_class_weighting(\n",
        "#     train_metadata=train_metadata,\n",
        "#     test_metadata=test_metadata,\n",
        "#     image_folder=image_folder,\n",
        "#     train_size=1000\n",
        "# )"
      ],
      "metadata": {
        "id": "sUj0IHqg3dgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_500_class, history_500_class, acc_500_class, auc_500_class = train_model_with_class_weighting(\n",
        "#     train_metadata=train_metadata,\n",
        "#     test_metadata=test_metadata,\n",
        "#     image_folder=image_folder,\n",
        "#     train_size=500\n",
        "# )"
      ],
      "metadata": {
        "id": "oh4T3ro8ZJWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_with_noise(train_metadata, test_metadata, image_folder,\n",
        "                            train_size,\n",
        "                            image_size=(224, 224), batch_size=32, epochs=5,\n",
        "                            output_dir=\"/content/drive/MyDrive/NIH_ChestXRay_Data_Neuro240/results_noise\"):\n",
        "    import os\n",
        "    import pandas as pd\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve\n",
        "    from tensorflow.keras.models import Sequential\n",
        "    from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "    from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "    # Sample training and test subsets\n",
        "    train_metadata_subset = train_metadata.sample(train_size, random_state=42)\n",
        "    test_metadata_subset = test_metadata.sample(int(train_size / 4), random_state=42)\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    result_prefix = os.path.join(output_dir, f\"train_{train_size}\")\n",
        "\n",
        "    # Save class distributions\n",
        "    def save_distribution(df, name):\n",
        "        counts = df[\"Finding Labels\"].value_counts()\n",
        "        percents = counts / counts.sum() * 100\n",
        "        dist_df = pd.DataFrame({\"Count\": counts, \"Percent\": percents})\n",
        "        dist_df.to_csv(f\"{result_prefix}_{name}_distribution.csv\")\n",
        "        return dist_df\n",
        "\n",
        "    train_dist = save_distribution(train_metadata_subset, \"train\")\n",
        "    test_dist = save_distribution(test_metadata_subset, \"test\")\n",
        "\n",
        "    # Combined barplot\n",
        "    combined_df = pd.concat([\n",
        "        train_dist[\"Percent\"].rename(\"Train\"),\n",
        "        test_dist[\"Percent\"].rename(\"Test\")\n",
        "    ], axis=1).fillna(0).reset_index()\n",
        "\n",
        "    # Rename the correct column to 'Label' (usually the first column)\n",
        "    combined_df.columns.values[0] = \"Label\"\n",
        "\n",
        "    combined_df = pd.melt(combined_df, id_vars=\"Label\", var_name=\"Set\", value_name=\"Percent\")\n",
        "\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.barplot(data=combined_df, x=\"Label\", y=\"Percent\", hue=\"Set\")\n",
        "    plt.title(\"Class Distribution: Train vs Test\")\n",
        "    plt.ylabel(\"Percentage\")\n",
        "    plt.xlabel(\"Class Label\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{result_prefix}_class_distribution_comparison.png\")\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "    # Data generators with noisey training set\n",
        "    train_datagen_noisy = ImageDataGenerator(\n",
        "        rescale=1.0 / 255,\n",
        "        rotation_range=20,\n",
        "        width_shift_range=0.15,\n",
        "        height_shift_range=0.15,\n",
        "        horizontal_flip=True,\n",
        "        brightness_range=[0.8, 1.2],\n",
        "        zoom_range=0.1,\n",
        "        preprocessing_function=lambda x: x + np.random.normal(loc=0.0, scale=0.05, size=x.shape)\n",
        "    )\n",
        "    test_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
        "\n",
        "    train_generator = train_datagen.flow_from_dataframe(\n",
        "        dataframe=train_metadata_subset,\n",
        "        directory=image_folder,\n",
        "        x_col=\"Image Index\",\n",
        "        y_col=\"Finding Labels\",\n",
        "        target_size=image_size,\n",
        "        batch_size=batch_size,\n",
        "        class_mode=\"binary\",\n",
        "        shuffle=True\n",
        "    )\n",
        "    test_generator = test_datagen.flow_from_dataframe(\n",
        "        dataframe=test_metadata_subset,\n",
        "        directory=image_folder,\n",
        "        x_col=\"Image Index\",\n",
        "        y_col=\"Finding Labels\",\n",
        "        target_size=image_size,\n",
        "        batch_size=batch_size,\n",
        "        class_mode=\"binary\",\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    # CNN model\n",
        "    model = Sequential([\n",
        "        Conv2D(32, (3, 3), activation=\"relu\", input_shape=(224, 224, 3)),\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "        Conv2D(64, (3, 3), activation=\"relu\"),\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "        Conv2D(128, (3, 3), activation=\"relu\"),\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "        Flatten(),\n",
        "        Dense(128, activation=\"relu\"),\n",
        "        Dropout(0.5),\n",
        "        Dense(1, activation=\"sigmoid\")\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "    from sklearn.utils.class_weight import compute_class_weight\n",
        "    import numpy as np\n",
        "\n",
        "    # Map labels to integers using the generator's class indices\n",
        "    label_map = train_generator.class_indices\n",
        "    inv_label_map = {v: k for k, v in label_map.items()}\n",
        "\n",
        "    # Convert y labels to 0/1\n",
        "    y_train_labels = train_metadata_subset[\"Finding Labels\"].map(label_map)\n",
        "\n",
        "    # Compute class weights\n",
        "    # class_weights = compute_class_weight(\n",
        "    #     class_weight=\"balanced\",\n",
        "    #     classes=np.unique(y_train_labels),\n",
        "    #     y=y_train_labels\n",
        "    # )\n",
        "    # class_weight_dict = dict(enumerate(class_weights))\n",
        "\n",
        "    history = model.fit(\n",
        "        train_generator,\n",
        "        validation_data=test_generator,\n",
        "        epochs=epochs,\n",
        "        verbose=1\n",
        "        # class_weight=class_weight_dict\n",
        "    )\n",
        "\n",
        "    test_loss, test_acc = model.evaluate(test_generator)\n",
        "    y_pred_proba = model.predict(test_generator).flatten()\n",
        "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
        "    y_true = test_generator.classes\n",
        "    auc = roc_auc_score(y_true, y_pred_proba)\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    # Save confusion matrix\n",
        "    confusion_df = pd.DataFrame(\n",
        "    cm,\n",
        "    index=[\"Mass\", \"No Finding\"],\n",
        "    columns=[\"Predicted Mass\", \"Predicted No Finding\"])\n",
        "\n",
        "    confusion_df.to_csv(f\"{result_prefix}_confusion_matrix.csv\")\n",
        "\n",
        "    # Save predictions\n",
        "    filenames = test_generator.filenames\n",
        "    results_df = pd.DataFrame({\n",
        "        \"Filename\": filenames,\n",
        "        \"TrueLabel\": y_true,\n",
        "        \"PredictedLabel\": y_pred,\n",
        "        \"PredictedProb\": y_pred_proba\n",
        "    })\n",
        "    results_df.to_csv(f\"{result_prefix}_predictions.csv\", index=False)\n",
        "\n",
        "    # Save ROC curve\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, label=f\"AUC = {auc:.2f}\")\n",
        "    plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")\n",
        "    plt.xlabel(\"False Positive Rate\")\n",
        "    plt.ylabel(\"True Positive Rate\")\n",
        "    plt.title(\"ROC Curve\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.savefig(f\"{result_prefix}_roc_curve.png\")\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"Test Accuracy (train size={train_size}): {test_acc * 100:.2f}%\")\n",
        "    print(f\"AUC: {auc:.4f}\")\n",
        "\n",
        "    return model, history, test_acc, auc"
      ],
      "metadata": {
        "id": "8gfTzrG9PQqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_250, history_250, acc_250, auc_250 = train_model_with_noise(\n",
        "    train_metadata=train_metadata,\n",
        "    test_metadata=test_metadata,\n",
        "    image_folder=image_folder,\n",
        "    train_size=250\n",
        ")"
      ],
      "metadata": {
        "id": "9jaWelBSso7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_250, history_250, acc_250, auc_250 = train_model_with_noise(\n",
        "    train_metadata=train_metadata,\n",
        "    test_metadata=test_metadata,\n",
        "    image_folder=image_folder,\n",
        "    train_size=500\n",
        ")"
      ],
      "metadata": {
        "id": "ejdC5X1lsu5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_250, history_250, acc_250, auc_250 = train_model_with_noise(\n",
        "    train_metadata=train_metadata,\n",
        "    test_metadata=test_metadata,\n",
        "    image_folder=image_folder,\n",
        "    train_size=1000\n",
        ")"
      ],
      "metadata": {
        "id": "YDbv-HtAswHn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}